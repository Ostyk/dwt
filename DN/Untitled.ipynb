{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.io\n",
    "import scipy.io as sio\n",
    "import skimage.transform\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "def read_mat(path):\n",
    "    return np.load(path)\n",
    "\n",
    "def write_mat(path, m):\n",
    "    np.save(path, m)\n",
    "\n",
    "def read_ids(path):\n",
    "    return [line.rstrip('\\n') for line in open(path)]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "from train_direction import initialize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputChannels = 2\n",
    "classType = 'unified_CR'\n",
    "# 0 leaf --> background?\n",
    "indices = [0]\n",
    "savePrefix = \"direction_\" + classType + \"_unified_CR_pretrain\"\n",
    "batchSize = 2\n",
    "learningRate = 1e-5\n",
    "# learningRateActual = 1e-7\n",
    "wd = 1e-5\n",
    "initialIteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(outputChannels=outputChannels, wd=wd, modelWeightPaths=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeeder = Batch_Feeder(dataset_path=\"../../watershednet/data/for_training/42/\", \n",
    "                           indices=indices,\n",
    "                           subset='val',\n",
    "                           batchSize=batchSize,\n",
    "                           padWidth=None,\n",
    "                           padHeight=None, \n",
    "                           flip=False,\n",
    "                           keepEmpty=False,\n",
    "                           train=True,\n",
    "                           img_shape = (384,384))\n",
    "trainFeeder.set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeeder._paths[0].angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = trainFeeder.polygons[trainFeeder._paths[0].img]['polygons']\n",
    "ss = trainFeeder.load_mask(np.zeros((384,384,3)), polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valFeeder = Batch_Feeder(dataset_path=\"../../watershednet/data/for_training/42/\", \n",
    "                           indices=indices,\n",
    "                           subset='val',\n",
    "                           batchSize=batchSize,\n",
    "                           padWidth=None,\n",
    "                           padHeight=None, \n",
    "                           flip=False,\n",
    "                           keepEmpty=False,\n",
    "                           train=True,\n",
    "                           img_shape = (384,384))\n",
    "valFeeder.set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, outputChannels, learningRate, trainFeeder, valFeeder, modelSavePath=None, savePrefix=None, initialIteration=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import direction_model\n",
    "from ioUtils import *\n",
    "import math\n",
    "import lossFunction\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     tfBatchImages = tf.placeholder(\"float\", shape=[None, 384, 384, 3])\n",
    "#     tfBatchGT = tf.placeholder(\"float\", shape=[None, 384, 384, 2])\n",
    "#     tfBatchSS = tf.placeholder(\"float\", shape=[None, 384, 384])\n",
    "\n",
    "#     with tf.name_scope(\"model_builder\"):\n",
    "#         print (\"attempting to build model\")\n",
    "#         model.build(tfBatchImages, tfBatchSS)\n",
    "#         print (\"built the model\")\n",
    "        \n",
    "#     sys.stdout.flush()\n",
    "#     loss = lossFunction.angularErrorLoss(pred=model.output, gt=tfBatchGT, ss=tfBatchSS, outputChannels=outputChannels)\n",
    "\n",
    "#     angleError = lossFunction.angularErrorTotal(pred=model.output, gt=tfBatchGT, ss=tfBatchSS, outputChannels=outputChannels)\n",
    "#     numPredicted = lossFunction.countTotal(ss=tfBatchSS)\n",
    "#     #numPredictedWeighted = lossFunction.countTotalWeighted(ss=tfBatchSS, weight=tfBatchWeight)\n",
    "#     exceed45 = lossFunction.exceedingAngleThreshold(pred=model.output, gt=tfBatchGT,\n",
    "#                                                     ss=tfBatchSS, threshold=45.0, outputChannels=outputChannels)\n",
    "#     exceed225 = lossFunction.exceedingAngleThreshold(pred=model.output, gt=tfBatchGT,\n",
    "#                                                     ss=tfBatchSS, threshold=22.5, outputChannels=outputChannels)\n",
    "\n",
    "#     train_op = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss=loss)\n",
    "\n",
    "#     init = tf.initialize_all_variables()\n",
    "\n",
    "#     sess.run(init)\n",
    "#     iteration = initialIteration\n",
    "    \n",
    "    \n",
    "#     while iteration < 1000:\n",
    "#         batchLosses = []\n",
    "#         totalAngleError = 0\n",
    "#         totalExceed45 = 0\n",
    "#         totalExceed225 = 0\n",
    "#         totalPredicted = 0\n",
    "\n",
    "#         for k in tqdm(range(int(math.floor(valFeeder.total_samples() / batchSize)))):\n",
    "#             imageBatch, gtBatch, ssBatch = valFeeder.next_batch()\n",
    "\n",
    "#             batchLoss, batchAngleError, batchPredicted, batchExceed45, batchExceed225 = sess.run(\n",
    "#                 [loss, angleError, numPredicted, exceed45, exceed225],\n",
    "#                 feed_dict={tfBatchImages: imageBatch,\n",
    "#                            tfBatchGT: gtBatch,\n",
    "#                            tfBatchSS: ssBatch})\n",
    "#             # print \"ran iteration\"\n",
    "#             batchLosses.append(batchLoss)\n",
    "#             totalAngleError += batchAngleError\n",
    "#             totalPredicted += batchPredicted\n",
    "#             totalExceed45 += batchExceed45\n",
    "#             totalExceed225 += batchExceed225\n",
    "\n",
    "#         if np.isnan(np.mean(batchLosses)):\n",
    "#             print (\"LOSS RETURNED NaN\")\n",
    "#             sys.stdout.flush()\n",
    "#             break\n",
    "\n",
    "#         print (\"%s Itr: %d - val loss: %.3f, angle MSE: %.3f, exceed45: %.3f, exceed22.5: %.3f\" % (\n",
    "#             time.strftime(\"%H:%M:%S\"), iteration,\n",
    "#         float(np.mean(batchLosses)), totalAngleError / totalPredicted,\n",
    "#         totalExceed45 / totalPredicted, totalExceed225 / totalPredicted))\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "#         if (iteration > 0 and iteration % 5 == 0) or checkSaveFlag(modelSavePath):\n",
    "#             modelSaver(sess, modelSavePath, savePrefix, iteration)\n",
    "\n",
    "#         for j in range(int(math.floor(trainFeeder.total_samples() / batchSize))):\n",
    "#             # print \"running batch %d\"%(j)\n",
    "#             # sys.stdout.flush()\n",
    "#             imageBatch, gtBatch, ssBatch = trainFeeder.next_batch()\n",
    "#             sess.run(train_op, feed_dict={tfBatchImages: imageBatch,\n",
    "#                                           tfBatchGT: gtBatch,\n",
    "#                                           tfBatchSS: ssBatch})\n",
    "#         iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb         direction_model.py.bak train_direction.py\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m            ioUtils.py\n",
      "direction_model.py     lossFunction.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model=model, outputChannels=outputChannels,\n",
    "                learningRate=learningRate,\n",
    "                trainFeeder=trainFeeder, valFeeder=valFeeder,\n",
    "                modelSavePath=\"./cityscapes/models/direction\", savePrefix=savePrefix,\n",
    "                initialIteration=initialIteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Batch_Feeder:\n",
    "    def __init__(self, dataset_path, indices, subset, batchSize, padWidth=None, padHeight=None, flip=False, keepEmpty=True, train=True, img_shape=(384,384)):\n",
    "        \n",
    "        assert subset in ['train', 'val', 'test'], \"wrong name of subset\"\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        self._dataset_path = dataset_path\n",
    "        self._indices = indices\n",
    "        self._subset = subset\n",
    "        self._train = train\n",
    "        self._batchSize = batchSize\n",
    "        self._padWidth = padWidth\n",
    "        self._padHeight = padHeight\n",
    "        self._flip = flip\n",
    "        self._keepEmpty = keepEmpty\n",
    "        self.img_shape = img_shape\n",
    "        mask_path = os.path.join(self._dataset_path, 'polygons.json')\n",
    "        with open(mask_path) as f:\n",
    "            self.polygons = json.load(f)\n",
    "        \n",
    "        # TO DO: implement shuffling\n",
    "        # TO DO: support batch wise inference\n",
    "        \n",
    "\n",
    "    def set_paths(self):\n",
    "        self.root = os.path.join(self._dataset_path, self._subset)\n",
    "        print('scanning {}'.format(self.root))\n",
    "        self._paths = []\n",
    "        \n",
    "        imgs = sorted([i for i in os.listdir(self.root) if i.endswith('.png')])\n",
    "        gt_DT = sorted([i for i in os.listdir(self.root) if i.endswith('.npy') and 'DT' in i])\n",
    "        gt_angle = sorted([i for i in os.listdir(self.root) if i.endswith('.npy') and 'angle' in i])\n",
    "\n",
    "#         if self._train:\n",
    "\n",
    "        # TO DO: support batch wise inference\n",
    "\n",
    "        entry = namedtuple(\"gt\", \"index img angle dt\")\n",
    "        for index, (img, angle, dt) in enumerate(zip(imgs, gt_angle, gt_DT)):\n",
    "            self._paths.append(entry(index, img, angle, dt))\n",
    "\n",
    "             \n",
    "\n",
    "            self.shuffle()\n",
    "#         else:\n",
    "#             for id in idList:\n",
    "#                 self._paths.append([id, imageDir + '/' + id + '_leftImg8bit.png',\n",
    "#                                     ssDir + '/' + id + '_unified_ss.mat'])\n",
    "\n",
    "        self._numData = len(self._paths)\n",
    "\n",
    "        if self._numData < self._batchSize:\n",
    "            self._batchSize = self._numData\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self._paths)\n",
    "\n",
    "    def next_batch(self):\n",
    "        idBatch = []\n",
    "        \n",
    "        imageBatch = np.zeros((self._batchSize, self.img_shape[0], self.img_shape[1], 3), dtype=np.int32)\n",
    "        gtBatch = np.zeros((self._batchSize,  self.img_shape[0], self.img_shape[1]))\n",
    "        ssBatch = np.zeros((self._batchSize,  self.img_shape[0], self.img_shape[1]))\n",
    "        \n",
    "        tmp = 0\n",
    "        \n",
    "        if self._train:\n",
    "            while(len(idBatch) < self._batchSize):\n",
    "                \n",
    "                current_tuple = self._paths[self._index_in_epoch]\n",
    "                \n",
    "                rgb = self.load_rgb(os.path.join(self.root, current_tuple.img))\n",
    "        \n",
    "                #angle = self.load_npy(os.path.join(self.root, current_tuple.angle))\n",
    "                dt = self.load_npy(os.path.join(self.root, current_tuple.dt))\n",
    "                \n",
    "                polygons = self.polygons[current_tuple.img]['polygons']\n",
    "                mask = self.load_mask(rgb, polygons)\n",
    "\n",
    "\n",
    "                imageBatch[tmp] = rgb\n",
    "                gtBatch[tmp] = dt\n",
    "                ssBatch[tmp] = mask\n",
    "\n",
    "                idBatch.append(current_tuple.index)\n",
    "                \n",
    "                tmp+=1\n",
    "                if tmp==self._batchSize-1:\n",
    "                    tmp=0\n",
    "                self._index_in_epoch += 1\n",
    "                \n",
    "                if self._index_in_epoch == self._numData:\n",
    "                    self._index_in_epoch = 0\n",
    "                    self.shuffle()\n",
    "\n",
    "            if self._flip and np.random.uniform() > 0.5:\n",
    "                for i in range(len(imageBatch)):\n",
    "                    for j in range(3):\n",
    "                        imageBatch[i,:,:,j] = np.fliplr(imageBatch[i,:,:,j])\n",
    "\n",
    "                    weightBatch[i] = np.fliplr(weightBatch[i])\n",
    "                    ssBatch[i] = np.fliplr(ssBatch[i])\n",
    "                    ssMaskBatch[i] = np.fliplr(ssMaskBatch[i])\n",
    "\n",
    "                    for j in range(2):\n",
    "                        gtBatch[i,:,:,j] = np.fliplr(gtBatch[i,:,:,j])\n",
    "\n",
    "                    gtBatch[i,:,:,0] = -1 * gtBatch[i,:,:,0]\n",
    "            return imageBatch, gtBatch, ssBatch\n",
    "        else:\n",
    "            pass\n",
    "            self._index_in_epoch += self._batchSize\n",
    "            return imageBatch, ssBatch\n",
    "        \n",
    "    def total_samples(self):\n",
    "        return self._numData\n",
    "\n",
    "    def image_scaling(self, rgb_in):\n",
    "        if rgb_in.dtype == np.float32:\n",
    "            rgb_in = rgb_in*255\n",
    "        elif rgb_in.dtype == np.uint8:\n",
    "            rgb_in = rgb_in.astype(np.float32)\n",
    "\n",
    "        # VGG16 was trained using opencv which reads images as BGR, but skimage reads images as RGB\n",
    "        rgb_out = np.zeros(rgb_in.shape).astype(np.float32)\n",
    "        rgb_out[:,:,0] = rgb_in[:,:,2] - VGG_MEAN[2]\n",
    "        rgb_out[:,:,1] = rgb_in[:,:,1] - VGG_MEAN[1]\n",
    "        rgb_out[:,:,2] = rgb_in[:,:,0] - VGG_MEAN[0]\n",
    "\n",
    "        return rgb_out\n",
    "\n",
    "    def pad(self, data):\n",
    "        if self._padHeight and self._padWidth:\n",
    "            if data.ndim == 3:\n",
    "                npad = ((0,self._padHeight-data.shape[0]),(0,self._padWidth-data.shape[1]),(0,0))\n",
    "            elif data.ndim == 2:\n",
    "                npad = ((0, self._padHeight - data.shape[0]), (0, self._padWidth - data.shape[1]))\n",
    "            padData = np.pad(data, npad, mode='constant', constant_values=0)\n",
    "\n",
    "        else:\n",
    "            padData = data\n",
    "\n",
    "        return padData\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load_rgb(path):\n",
    "        return cv2.cvtColor(cv2.imread(path), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_mask(img, polygons):\n",
    "        \"\"\"\n",
    "        Transforms polygons of a single image into a 2D binary numpy array\n",
    "        \n",
    "        :param img: just to get the corresponding shape of the output array\n",
    "        :param polygons: - dict\n",
    "        \n",
    "        :return mask: numpy array with drawn over and touching polygons\n",
    "        \"\"\"\n",
    "        mask = np.zeros([img.shape[0], img.shape[1]], dtype=np.uint8)\n",
    "        for curr_pol in polygons:\n",
    "            cv2.fillPoly(mask, [np.array(curr_pol, 'int32')], 255)\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_npy(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            depth = np.load(f)\n",
    "        return depth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((trainFeeder._batchSize, trainFeeder.img_shape[0], trainFeeder.img_shape[1], 3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainFeeder = Batch_Feeder(dataset_path=\"../../watershednet/data/for_training/42/\", \n",
    "                           indices=indices,\n",
    "                           subset='val',\n",
    "                           batchSize=batchSize,\n",
    "                           padWidth=None,\n",
    "                           padHeight=None, \n",
    "                           flip=False,\n",
    "                           keepEmpty=False,\n",
    "                           train=True,\n",
    "                           img_shape = (384,384))\n",
    "trainFeeder.set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeeder._numData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tqdm(range(int(np.floor(trainFeeder.total_samples() / batchSize)))):\n",
    "    # print \"running batch %d\"%(j)\n",
    "    # sys.stdout.flush()\n",
    "    imageBatch, gtBatch, ssBatch = trainFeeder.next_batch()\n",
    "    print(imageBatch.shape, gtBatch.shape, ssBatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageBatch[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeeder._index_in_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((trainFeeder._batchSize, trainFeeder.img_shape[0], trainFeeder.img_shape[1]))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeeder._paths[40].img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_map =trainFeeder.load_npy(os.path.join(trainFeeder.root, trainFeeder._paths[40].angle))\n",
    "seg_get = trainFeeder.Polygon2Mask2D(angle_map.shape, trainFeeder._paths[40].angle)\n",
    "plt.imshow(seg_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sio.loadmat('species_38_das_6_image_1.png.json_slice_DT_0013.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = (sio.loadmat(self._paths[self._index_in_epoch][3])['mask']).astype(float)\n",
    "ssMask = ss\n",
    "ss = np.sum(ss[:,:,self._indices], 2)\n",
    "\n",
    "background = np.zeros(ssMask.shape[0:2] + (1,))\n",
    "ssMask = np.concatenate((ssMask[:,:,[1,2,3,4]], background, ssMask[:,:,[0,5,6,7]]), axis=-1)\n",
    "ssMask = np.argmax(ssMask, axis=-1)\n",
    "ssMask = ssMask.astype(float)\n",
    "ssMask = (ssMask - 4) * 32 # centered at 0, with 0 being background, spaced 32 apart for classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
